\documentclass{ieeeojies}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{array}
\usepackage{changepage}
\usepackage{wrapfig}
\usepackage{listings}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{float}

\lstset{
    basicstyle=\ttfamily,
    breaklines=true,
    frame=single,
    showstringspaces=false,
}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
 
\begin{document}
\title{TECHSTOCK FORECASTING: DECODING MARKET TRENDS WITH STATISTICAL MODELS AND MACHINE LEARNING}
\author{\uppercase{Le Ba Dac}\authorrefmark{1},
\uppercase{Do Le Hau\authorrefmark{2}, and Duong Ly Tuyet Mai}\authorrefmark{3}}
 
\address[1]{Faculty of Information Systems, University of Information Technology, (e-mail: 21521911@gm.uit.edu.vn)}
\address[2]{Faculty of Information Systems, University of Information Technology, (e-mail: 21522052@gm.uit.edu.vn)}
\address[3]{Faculty of Information Systems, University of Information Technology, (e-mail: 21522318@gm.uit.edu.vn)}

\markboth
{Author \headeretal: Ba Dac. Le, Le Hau. Do, Ly Tuyet Mai. Duong}
{Author \headeretal: Ba Dac. Le, Le Hau. Do, Ly Tuyet Mai. Duong}
\begin{abstract}
Forecasting stock market trends is a critical aspect of investment decision-making, particularly in the fast-paced and dynamic world of technology stocks. This scientific report examines the application of statistical models and machine learning techniques for predicting the future movements of three specific technology stocks in Vietnam. By leveraging the power of data-driven approaches, this research aims to identify the most effective and accurate methods for forecasting tech stock prices. Eight different forecasting models are evaluated in this study: ARIMA, SVR, GRU, linear regression, GARCH, KNN, CNN-GRU, BNN. Through a comprehensive analysis and comparison of these models, their performances are assessed to determine the most reliable and precise approach for predicting tech stock prices.
\end{abstract}

\begin{keywords}
tech stocks in Vietnam, stock market forecasting, statistical models, machine learning, neural network, ARIMA, SVR, GRU, linear regression, GARCH, KNN, CNN-GRU, BNN.
\end{keywords}

\titlepgskip=-15pt

\maketitle

\section{Introduction}
\label{sec:introduction}
This research focuses on exploring and applying various machine learning models for stock prediction. The models examined in this study include ARIMA, SVR, GRU, Linear Regression (multivariate), GARCH, K-NN, and CNN-GRU.

In the realm of investment and finance, accurately predicting stock prices is crucial for making informed trading decisions. While traditional approaches such as fundamental and technical analysis have been employed, the emergence of machine learning has presented new opportunities for more advanced and precise predictions.

It is important to note that each machine learning model possesses its own unique characteristics and operational methods, encompassing data preprocessing, model construction, optimization techniques, and performance evaluation. In this thesis, we will delve into these models and assess their effectiveness in predicting stock prices.

The objective of this research is to thoroughly evaluate and compare the strengths, limitations, and performance of the aforementioned machine learning models. We will examine their adaptability to stock market data, their ability to forecast in volatile and uncertain market conditions, and their capacity to handle temporal factors and data correlations.

By conducting a comprehensive analysis and comparison of these machine learning models, this study aims to contribute to enhancing the accuracy of stock predictions and facilitating well-informed decision-making in the investment and finance domain

\section{Related research}
\label{sec:related research}
In recent years, there has been a significant amount of research focused on predicting stock prices using various machine learning and statistical models.
ARIMA (Autoregressive Integrated Moving Average) is a popular time series forecasting model that has been widely used for predicting stock prices. Researchers, such as Box and Jenkins (1970), have applied ARIMA to capture the autoregressive and moving average components of the data[1].
Suykens and Vandewalle (1999), have demonstrated the effectiveness of SVR in capturing complex relationships between input features and stock prices[2]. SVR has shown promising results in terms of accuracy and robustness.
Recurrent Neural Networks (RNNs), particularly the Gated Recurrent Unit (GRU) model proposed by Cho et al. (2014)[3], have been extensively explored for stock price prediction. Several researchers have applied GRU models to predict stock prices and have obtained competitive results.
Linear Regression, a simple yet powerful statistical model, has been widely used in stock price prediction. Researchers have utilized linear regression to model the linear relationship between input features and stock prices[4].
GARCH (Generalized Autoregressive Conditional Heteroskedasticity) models have been extensively employed to capture the volatility clustering and time-varying characteristics of stock prices. Researchers, such as Bollerslev (1986), have applied GARCH models to predict stock market volatility, which is useful for risk management and trading strategies[5].
K-Nearest Neighbors (KNN) is a non-parametric algorithm that has found applications in stock price prediction. Researchers have utilized KNN to identify similar historical patterns and make predictions based on the nearest neighbors. KNN is effective in capturing local trends and patterns in the data[6].
Recently, the combination of Convolutional Neural Networks (CNNs) and RNNs, known as CNN-GRU, has gained attention in stock price prediction. CNNs are employed to extract local features from stock price sequences, which are then fed into GRU models to capture temporal dependencies. This hybrid approach has shown promise in capturing both local and global patterns in stock price data[7].
Bayesian Neural Networks (BNNs) provide a probabilistic framework that can capture prediction uncertainty and improve robustness. Researchers have explored various Bayesian inference techniques to train BNNs for stock price prediction tasks[8].
In conclusion, a wide range of models, including ARIMA, SVR, GRU, Linear Regression, GARCH, KNN, CNN-GRU, and BNN, have been utilized in predicting stock prices. Each model has its own strengths and limitations, and the choice of model depends on the specific characteristics of the dataset and the objectives of the prediction task. In this study, our aim is to compare and evaluate the performance of these models in predicting stock prices and propose novel approaches to enhance prediction accuracy.

\section{Materials}
\label{sec:materials}
\subsection{Data source}
In this study, we gathered three datasets pertaining to stock trends in the technology sector of Vietnam, focusing on three companies: FPT Corporation, Elcom Technology Communications Corp, and CMC Corporation. The data were extracted from Investing.com, covering the period from December 15th, 2016, to December 15th, 2023. Each raw dataset comprises approximately 1750 rows and encompasses seven attributes, namely Date, Close Price, Open Price, High Price, Low Price, Trading Volume, and Change. However, for the purpose of this paper, we meticulously processed the data, retaining only the relevant columns.

\subsection{Description statistics} 


\begin{adjustwidth}{+0.7cm}{}

\begin{tabular}{|c|c|c|c|c|}
\hline
Name & FPT & ELC & CMG \\
\hline
count & 1750 & 1749 & 1744\\
\hline
mean & 48034.80 & 11718.10 & 29648.12 \\
\hline
std & 26937.97 & 6039.30 & 14000.22 \\
\hline
min & 15077.40 & 3409.00 & 8729.30\\
\hline
25\% & 24013.73 & 5900.30 & 17771.15\\
\hline
50\% & 34173.10 & 11063.00 & 27772.05\\
\hline
75\% & 77845.00 & 16165.10 & 41500.00\\
\hline
max & 99000.00 & 27300.00 & 62430.00\\
\hline  
\end{tabular}
\end{adjustwidth}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{bibliography/FPTBP.png}
    \caption{FPT Stock Price's Boxplot}
    \label{fig:1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{bibliography/FPTHis.png}
    \caption{FPT Stock Price's Histogram}
    \label{fig:2}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{bibliography/ELCBP.png}
    \caption{ELC Stock Price's Boxplot}
    \label{fig:3}
    \end{minipage}
    \hfill
    \begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{bibliography/ELCHis.png}
    \caption{ELC Stock Price's Histogram}
    \label{fig:4}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{bibliography/CMGBP.png}
    \caption{CMG Stock Price's Boxplot}
    \label{fig:3}
    \end{minipage}
    \hfill
    \begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{bibliography/CMGHis.png}
    \caption{CMG Stock Price's Histogram}
    \label{fig:4}
    \end{minipage}
\end{figure}

\section{Methodology}
\label{sec:Methodology}

\subsection{Linear Regression}
Multivariable linear regression, also known as multiple linear regression, is a statistical modeling technique used to analyze the relationship between multiple independent variables (also called predictors or features) and a single dependent variable (also known as the target or response variable). It extends the concept of simple linear regression, which deals with only one independent variable.\\
In multivariable linear regression, the goal is to estimate the coefficients or weights that represent the linear relationship between the independent variables and the dependent variable. \\
Formula:

\begin{quote}
y=$\beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n + \epsilon_0$
\end{quote}
Where:
\begin{quote}
    + y is the dependent variable.\\
    + $\beta_0$ is the intercept. \\
    + $\beta_1$,$\beta_2$, ..., $\beta_n$ are the coefficients of $x_1$,$x_2$,…,$x_n$.\\
    + $x_1$, $x_2$, …, $x_n$ are the independent variables.
\end{quote}


\subsection{SVR}
Support Vector Regression (SVR) is a machine learning algorithm modified from Support Vector Machines (SVM) that is used for regression problems instead of classification.\\
SVR aims to construct an appropriate regression model by finding a hyperplane in the feature space. This hyperplane is chosen such that the distance between the data points and the hyperplane is minimized. The data points closest to the hyperplane are referred to as support vectors.\\\
To determine the optimal model, SVR performs the optimization process by solving a loss function with constraints. These constraints ensure that the distance between the data points and the hyperplane is less than or equal to a predefined limit. 
\begin{table}[H]
    \centering
\begin{tabular}{|c|c|c|c|}
\hline
Name & Equation & Coefficient \\
\hline
linear & $x^Tz$ &  None\\
\hline
\centering polynomial & $(r + \gamma x^Tz)^d$  & \parbox{1.6cm}{\centering $d$: degree,\\ $\gamma$: gamma,\\ $r$: coef0} \\
\hline
sigmoid & \centering $\tanh(\gamma x^Tz + r)$ & \parbox{3cm}{\centering $\gamma$: gamma, \\ $r$: coef0} \\
\hline
rbf & $exp(-\gamma||x - z||_2^2)$ &$\gamma>0$: gamma \\
\hline
\end{tabular}
    \caption{SVR's kernel}
    \label{SVRkernel}
\end{table}

\subsection{Arima}
The ARIMA model predicts future values based on its current and past value of time series data. It combines the autoregressive (AR(p)), differencing (I(d)), and moving average (MA(q)) components to create an ARIMA (p,d,q) model. 
Below is a pseudocode representation of the ARIMA process based on the Box-Jenkins method, which encompasses stationarity testing, hyperparameter determination, hyperparameter tuning, and forecasting. Let's explore the pseudocode in detail:
\begin{lstlisting}[basicstyle=\footnotesize, escapeinside={(*@}{@*)}]
best_model, best_aic = None, \infty

while Input Data exists:
   Test for Stationarity using ADF
   if stationary:
       Determine d 
       Determine p using pACF
       Determine q using ACF
       # Hyperparameter Tuning Loop
       for each set of hyperparameters:
           current_model = Estimate ARIMA model
           Evaluate model performance (AIC)
           if AIC < best_aic:
               best_aic = AIC
               best_model = current_model
       # Forecasting using the best model
       Forecast future values with best ARIMA model
       (*@$\Delta^d Y_t = \mu + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} +...+\phi_p Y_{t-p}+\theta_1\epsilon_{t-1}+\theta_2 \epsilon_{t-2}+...+\theta_q \epsilon_{t-q} + \epsilon$@*)  
       break
   else:
       Apply transformations for stationarity
       Continue to the next iteration
\end{lstlisting}

\subsection{GRU}
GRU (Gated Recurrent Unit), is a type of Recurrent Neural Network (RNN). It is particularly effective in handling sequential data for tasks like time series prediction, natural language processing, and speech recognition.\\
The GRU architecture incorporates two essential components: the update gate and the reset gate. The update gate determines how much past information should be retained and passed on to future steps, enabling the model to effectively capture long-term dependencies. The reset gate, on the other hand, determines the significance given to previous information, enabling accurate predictions. These gates are represented as vectors with values ranging from 0 to 1, calculated using the sigmoid activation function. A value closer to 0 indicates a closed gate, meaning no information is transmitted, while a value closer to 1 indicates an open gate, allowing all information to be passed through.\\
The operations within a GRU can be described by the following set of equations: \\
\\Update Gate: \(\quad & \text{$z_t = \sigma(W_z \cdot [h_{t-1}, x_t])$}\) \\
Reset Gate: \(\quad & \text{$r_t = \sigma(W_r \cdot [h_{t-1}, x_t])$}\) \\
Candidate Hidden State: \(\quad & \text{$\hat{h}_t = \tanh(W \cdot [r_t \circ h_{t-1}, x_t])$}\)
Final Hidden State: \(\quad & \text{$h_t = (1 - z_t) \circ h_{t-1} + z_t \circ \hat{h}_t$}\)


\begin{figure}[H]
    \centering
    \begin{minipage}{1\linewidth}
        \centering
         \includegraphics[scale=0.22]{GRUarchitecture.jpg}
        \caption{GRU architecture}
        \label{fig:1}
    \end{minipage}
\end{figure}


\subsection{GARCH}
GARCH is a widely used model for estimating and forecasting volatility in financial markets. Unlike traditional models, GARCH takes into account the time-varying nature of volatility, which is essential for capturing the changing risk levels in financial markets.\\
\vspace{1pt}\\
GARCH incorporates past squared returns and volatility to predict future volatility using the following formula:
\begin{quote}
\[\sigma_t^2 = \omega + \sum_{i=1}^{q}\alpha_i\epsilon_{t-i}^2 + \sum_{j=1}^{p}\beta_j\sigma_{t-j}^2
\]
\end{quote}
Where:
\begin{quote}
   + $\sigma_t^2$ is the conditional variance (volatility) at time t.\\
   + $\omega$ is a constant term (baseline volatility).\\
   + $\alpha_i$ is the coefficient of the lagged squared error term ($\epsilon_{t-i}^2$), capturing the impact of past shocks on current volatility.\\
   + $\beta_j$ is the coefficient of the lagged conditional variance ($\sigma_{t-j}^2$), reflecting the persistence of volatility.

\end{quote}

\subsection{K-NN}
The K-Nearest Neighbor Algorithm is a straightforward machine learning algorithm that operates on the concept that similar samples tend to be located close to each other. It belongs to the category of instance-based learning methods, which are sometimes referred to as lazy learners. This is because they store all the training samples and only construct a classifier when a new, unlabeled sample needs to be classified.
\begin{figure}[H]
  \centering
  \begin{minipage}{1\linewidth}
    \centering
    \includegraphics[scale=0.75]{bibliography/KNN.png}
    \caption{KNN's example}
    \label{fig:3}
  \end{minipage}
\end{figure}

The measurement method of "distance" between two points:
\begin{quote}
- Euclidian’s distance: $\sqrt{\sum_{k-1}^d(x_{ik}-x_{jk})^2}$ \\
- Manhattan’s distance: $\sum_{k=1}^d\lvert x_{ik}-x{jk}\rvert$
\end{quote}

\subsection{CNN-GRU}
Convolutional Neural Network-Recurrent Neural Network (CNN-GRU), is a powerful neural network architecture that combines the strengths of both CNNs and RNNs. CNNs are effective in extracting spatial features from data like images and videos, while RNNs excel in handling sequential or temporal data such as audio and time series signals.\\
\vspace{1pt}\\
The CNN-GRU model consists of two main components:\\

CNN Model: is responsible for extracting spatial features from the input data. It applies convolutional layers to capture local patterns and uses pooling layers to reduce spatial dimensions while retaining important information. The output of the CNN model is a set of learned features.
\begin{figure}[H]
  \centering
  \begin{minipage}{1\linewidth}
    \centering
    \includegraphics[scale=0.4]{bibliography/CNN.png}
    \caption{CNN Network's example}
    \label{fig:3}
  \end{minipage}
\end{figure}
The convolution operation is expressed mathematically as follows:
\[
h_i=\sigma ( \sum_{j=1}^n \boldsymbol{W}_j \cdot x_{i+j}+b )
\]
Where:
\begin{quote}
   + $h_i$ represents the output feature map at the position i. \\
   + $\boldsymbol{W}$ denotes the weights of the $j^{th}$ filter.\\
   + $x_{i+j}$ represents the input data at position i+j\\
   + $b$ is the bias term.\\
   + $\sigma$ represents the activation function, such as the sigmoid function.\\
\end{quote}


GRU Model: takes the learned features from the CNN model as input and processes them over time. The GRU model utilizes recurrent connections and hidden states to capture temporal dependencies and relationships between data points.

\subsection{BNN}
In Bayesian Neural Networks (BNN), a distinctive feature is that each parameter in the model is not merely represented by a fixed value but is instead characterized through a posterior probability distribution.
A neural network based on Bayes backpropagation is shown in the below figure, where w is the neural network weight and the black curves are the neural network connections.
\begin{figure}[H]
  \centering
  \begin{minipage}{1\linewidth}
    \centering
     \includegraphics[scale=0.25]{bibliography/BNN.png}
        \caption{Neural network based on bayes backpropagation.}
    \label{fig:3}
  \end{minipage}
\end{figure}
The Bayesian backpropagation process is the key procedure for updating and adjusting the posterior probability distribution of parameters. It not only aids the model in learning from data but also retains uncertainty in predictions, making the model robust in dealing with ambiguous situations.\\
At the heart of BNNs lies Bayes' theorem, a guiding equation for updating our beliefs about model parameters. In the Bayesian Neural Network context, it takes the form:
\[
P(w|D) \propto P(D|w)P(w)
\]
Where:
\begin{quote}
    + P(w|D) is the posterior distribution of weights given data.\\
    + P(D|w) is the likelihood of the data given the weights.\\
    + P(w) is the prior belief in the weights.\\
\end{quote}
In practice, obtaining the true posterior  is often elusive. Enter variational inference, employing a variational distribution  to approximate the posterior:
\[
\theta^*=argminKL[q({\boldsymbol{w}|\theta})||P(\boldsymbol{w}|D)]
\]
\[
       =argmin\int q(\boldsymbol{w}|\theta)log\frac{q(\boldsymbol{w}|\theta)}{P(\boldsymbol{w})P(D|\boldsymbol{w})}dw
\]
\[
       =argminKL[q(\boldsymbol{w}|\theta)||P(\boldsymbol{w})] - E_{q(\boldsymbol{w}|\theta)}[log(D|\boldsymbol{w})]
\]
This involves minimizing the Kullback-Leibler (KL) divergence, quantifying how $q({w|\theta})$ deviates from P(w|D). \\
The BNN model can be represented as shown in the following figure
\begin{figure}[H]
  \centering
  \begin{minipage}{1\linewidth}
    \centering
     \includegraphics[scale=0.33]{BNN.png}
        \caption{BNN model}
    \label{fig:3}
  \end{minipage}
\end{figure}
\section{RESULT}

To assess the predictive efficacy of our proposed models, we utilize three key performance metrics commonly employed in scientific papers: Root Mean Square Error (RMSE), Mean Absolute Percentage Error (MAPE), and Mean Absolute Error (MAE). By incorporating these three metrics into our evaluation, we gain valuable insights into the performance of our models. RMSE offers a comprehensive overview of overall accuracy, while MAPE evaluates percentage deviation, and MAE captures the absolute prediction errors. The inclusion of results from these metrics in our scientific paper enables us to objectively compare and analyze the performance of different models. The ultimate goal is to identify the best-performing model that achieves optimal prediction accuracy based on these comprehensive evaluations.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
         \hline
         \multicolumn{5}{|c|}{\textbf{FPT Stock Price's Evaluation}}\\
         \hline
         \centering Model & Proportion & RMSE & MAPE (\%) & MAE\\
         \hline
         \multirow{2}{*}{LN} & 7:3 & 16295.71 & 17.66 & 14839.82 \\ & \textbf{8:2} & \textbf{5326.88} & \textbf{4.92} & \textbf{4108.75} \\ & 9:1 & 4899.28 & 4.47 & 3877.19\\
         \hline
         \multirow{2}{*}{SVR} & 7:3 &14762.56 & 10.413&9367.34\\ & \textbf{8:2} &\textbf{4061.46} & \textbf{2.41}& \textbf{2198.06}\\ & 9:1 &2963.25  &3.16  &4578.09 \\
         \hline
         \multirow{2}{*}{GRU} & 7:3	& 462.042& 1.6 &345.278  \\ & 8:2 &390.319 &  1.4&292.389  \\ & \textbf{9:1} &\textbf{223.969}   & \textbf{1} &\textbf{173.577}\\
         \hline
         \multirow{2}{*}{ARIMA} & 7:3 &  27215.81 & 28.82 & 23706.79 \\ & 8:2 &  6914.88 & 6.78 & 5595.62 \\ & \textbf{9:1} & \textbf{5604.60}  & \textbf{4.8331} & \textbf{4349.27}\\
         \hline
         \multirow{2}{*}{GARCH} & 7:3	& 2.47 & 341.24 & 1.91 \\ & \textbf{8:2} & \textbf{2.24} & \textbf{347.13} & \textbf{1.73} \\ & 9:1 & 2.25 & 379.12 & 1.67\\
         \hline
         \multirow{2}{*}{K-NN} & 7:3 & \multicolumn{3}{c|}{Accuracy: 99,81}  \\ & 8:2 & \multicolumn{3}{c|}{\textbf{Accuracy: 100}}   \\ & 9:1 & \multicolumn{3}{c|}{Accuracy: 100}   \\
         \hline
         \multirow{2}{*}{CNN-GRU} & 7:3 &1624.02  & 1.28  &1065.81  \\ & \textbf{8:2} & \textbf{1449.27} &\textbf{1.17} & \textbf{957.45} \\ & 9:1 &  1651.39 &1.19	 & 1046.31	 \\
         \hline
         \multirow{2}{*}{BNN} & 7:3 &  2293.89 & 2.05  & 1712.06 \\ & \textbf{8:2} & \textbf{1667.02}  & \textbf{1.27} & \textbf{1094.73}\\ & 9:1 & 1932.64 & 1.67 & 1538.05\\
         \hline
    \end{tabular}
    \caption{FPT Stock Price's Evaluation}
    \label{FPTresult}
\end{table}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
         \hline
         \multicolumn{5}{|c|}{\textbf{ELC Stock Price's Evaluation}}\\
         \hline
         \centering Model & Proportion & RMSE & MAPE (\%) & MAE\\
         \hline
         \multirow{2}{*}{LN} & 7:3 &11708.03 &59.64 & 10494.62 \\ & \textbf{8:2}  & \textbf{5988.27} & \textbf{24.67} & \textbf{4222.53} \\ & 9:1 & 8398.24&32.21 &6914.58 \\
         \hline
         \multirow{2}{*}{SVR} & 7:3 & 532.44 &1.69 &295.08\\ & \textbf{8:2} &\textbf{486.47} &\textbf{1.64} &\textbf{268.63} \\ & 9:1 & 656.86 &1.70  & 368.93\\
         \hline
         \multirow{2}{*}{GRU} &7:3 	&262.388 &1.5  &179.346  \\ & 8:2 &345.676  &1.8  & 270.861 \\ & \textbf{9:1} & \textbf{208.03}  & \textbf{1.2} &\textbf{209.198}\\
         \hline
         \multirow{2}{*}{ARIMA} & 7:3 & 34285.67  &17.50 &27492.33  \\ & \textbf{8:2} & \textbf{5477.97} & \textbf{25.50} & \textbf{3948.54} \\ & 9:1 &7073.14   & 23.66 &5353.74 \\
         \hline
         \multirow{2}{*}{GARCH} & 7:3	&4.56 &314.15 &3.68  \\ & 8:2 &4.32  & 310.79 &3.44  \\ & \textbf{9:1} &\textbf{4.04}  &\textbf{323.80} & \textbf{3.08}\\
         \hline
         \multirow{2}{*}{K-NN} & 7:3 & \multicolumn{3}{c|}{\textbf{Accuracy: 99,05}}  \\ & 8:2 & \multicolumn{3}{c|}{Accuracy: 98,57}   \\ & 9:1 & \multicolumn{3}{c|}{Accuracy: 98,56}   \\
         \hline
         \multirow{2}{*}{CNN-GRU} & \textbf{7:3} &\textbf{585.78}  &\textbf{2.45}  & \textbf{385.30} \\ & 8:2 & 591.06 &2.45 &  366.41\\ & 9:1 &1325.17  	 & 5.76& 1153.25	 \\
         \hline
         \multirow{2}{*}{BNN} & 7:3 &890.50   & 3.93 &626.10 \\ & \textbf{8:2} & \textbf{776.14} &\textbf{3.24} & \textbf{534.50}\\ & 9:1& 1188.13 & 3.22  &765.09 \\
         \hline
    \end{tabular}
    \caption{ELC Stock Price's Evaluation}
    \label{ELCresult}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
         \hline
         \multicolumn{5}{|c|}{\textbf{CMG Stock Price's Evaluation}}\\
         \hline
         \centering Model & Proportion & RMSE & MAPE (\%) & MAE\\
         \hline
         \multirow{2}{*}{LN} & 7:3 &3440.63 &17.13 &2596.34  \\ & \textbf{8:2}  & \textbf{4062.26} & \textbf{17.74} &\textbf{2750.59}  \\ & 9:1 &899.57 &8.15 &792.85 \\
         \hline
         \multirow{2}{*}{SVR} & 7:3 &5127.04  &3.76 &2089.53\\ & 8:2 & 704.06&0.29 &161.34 \\ & \textbf{9:1} &\textbf{28.40}  &\textbf{0.05}  &\textbf{22.54}\\
         \hline
         \multirow{2}{*}{GRU} & 7:3 	&637.11& 2.5 &  391.71\\ & 8:2 & 656.73 & 2.4 & 379.66 \\ & \textbf{9:1} & \textbf{217.27}  & \textbf{1.6} &\textbf{154.45}\\
         \hline
         \multirow{2}{*}{ARIMA} & 7:3 & 15382.80  & 30.38 & 13153.93 \\ & 8:2 &9577.45  & 20.32 & 8440.76 \\ &\textbf{9:1} & \textbf{5520.31}  &\textbf{ 9.96} &\textbf{4751.36}\\
         \hline
         \multirow{2}{*}{GARCH} & 7:3	&3.59 & 362.45&2.77\\ & 8:2 & 3.52 &395.46  & 2.61 \\ & \textbf{9:1} &\textbf{2.89}  &\textbf{447.54} &\textbf{2.05} \\
         \hline
         \multirow{2}{*}{K-NN} & \textbf{7:3} & \multicolumn{3}{c|}{\textbf{Accuracy: 99.81}}  \\ & 8:2 & \multicolumn{3}{c|}{Accuracy: 99,43}   \\ & 9:1 & \multicolumn{3}{c|}{Accuracy: 99,43}   \\
         \hline
         \multirow{2}{*}{CNN-GRU} & 7:3 & 1296.13 & 1.87 & 870.33 \\ & 8:2 & 1199.99 &1.94 &829.53  \\ & \textbf{9:1} &  \textbf{1125.47}	 & \textbf{1.29}& \textbf{583.05}	 \\
         \hline
         \multirow{2}{*}{BNN} & 7:3 & 1834.33  & 3.45 &1527.19 \\ & \textbf{8:2} &\textbf{1069.77}  & \textbf{1.59} & \textbf{706.76}\\ & 9:1& 2001.05 & 2.96  &1427.13 \\
         \hline
    \end{tabular}
    \caption{CMG Stock Price's Evaluation}
    \label{CMGresult}
\end{table}

\subsection{Predict the next 30 days}
- \textbf{Linear Resgression (Multivariables)} \\
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/LNFPT.png}
    \caption{Linear Resgression for FPT dataset}
    \label{fig:}
  \end{minipage}
\end{figure}
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/LNELC.png}
    \caption{Linear Resgression for ELC dataset}
    \label{fig:}
  \end{minipage}
\end{figure}
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/LNCMG.png}
    \caption{Linear Resgression for CMG dataset}
    \label{fig:}
  \end{minipage}
\end{figure}
- \textbf{SVR}\\
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/SVRFPT.png}
    \caption{SVR for FPT dataset}
    \label{fig:}
  \end{minipage}
\end{figure}
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/SVRELC.png}
        \caption{SVR for ELC dataset}
        \label{fig:}
  \end{minipage}
\end{figure}
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/SVRCMG.png}
        \caption{SVR model for CMG dataset}
        \label{fig:}
  \end{minipage}
\end{figure}

- \textbf{GRU}\\
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/GRUFPT.png}
        \caption{GRU for FPT dataset}
    \label{fig:}
  \end{minipage}
\end{figure}
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/GRUELC.png}
            \caption{GRU for ELC dataset}
    \label{fig:}
  \end{minipage}
\end{figure}
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/GRUCMG.png}
            \caption{GRU for CMG dataset}
    \label{fig:}
  \end{minipage}
\end{figure}

- \textbf{ARIMA}\\
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/ARIMAFPT.png}
            \caption{ARIMA for FPT dataset}
    \label{fig:}
  \end{minipage}
\end{figure}
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/ARIMAELC.png}
                \caption{ARIMA for ELC dataset}
    \label{fig:}
  \end{minipage}
\end{figure}
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/ARIMACMG.png}
                \caption{ARIMA for CMG dataset}
    \label{fig:}
  \end{minipage}
\end{figure}

- \textbf{GARCH}\\
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/GARCHFPT.png}
    \caption{GARCH for FPT dataset}
    \label{fig:}
  \end{minipage}
\end{figure}
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/GARCHELC.png}
        \caption{GARCH for ELC dataset}
    \label{fig:}
  \end{minipage}
\end{figure}
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/GARCHCMG.png}
        \caption{GARCH for CMG dataset}
    \label{fig:}
  \end{minipage}
\end{figure}

- \textbf{K-NN}\\
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/KNNFPT.png}
    \caption{K-NN for FPT dataset}
    \label{fig:}
  \end{minipage}
\end{figure}
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/KNNELC.png}
    \caption{K-NN for ELC dataset}
    \label{fig:}
  \end{minipage}
\end{figure}
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/KNNCMG.png}
    \caption{K-NN for CMG dataset}
    \label{fig:}
  \end{minipage}
\end{figure}

- \textbf{CNN - GRU}\\
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/CNNGRUFPT.png}
    \caption{CNN-GRU for FPT dataset}
    \label{fig:}
  \end{minipage}
\end{figure}
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/CNNGRUELC.png}
        \caption{CNN-GRU for ELC dataset}
    \label{fig:}
  \end{minipage}
\end{figure}
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/CNNGRUCMG.png}
        \caption{CNN-GRU for CMG dataset}
    \label{fig:}
  \end{minipage}
\end{figure}

- \textbf{BNN}\\
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/BNNFPT.png}
        \caption{BNN for FPT dataset}
    \label{fig:}
  \end{minipage}
\end{figure}
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/BNNELC.png}
            \caption{BNN for ELC dataset}
    \label{fig:}
  \end{minipage}
\end{figure}
\begin{figure}[H]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{bibliography/BNNCMG.png}
            \caption{BNN for CMG dataset}
    \label{fig:}
  \end{minipage}
\end{figure}

\section{Conclusion}
\subsection{Sumary}
In conclusion, this research delved into the application of various machine learning models for stock prediction in the dynamic and complex world of financial markets. The models examined in this study, including ARIMA, SVR, GRU, Linear Regression (multivariate), GARCH, K-NN, CNN-GRU, and BNN  provided valuable insights into the potential of machine learning in forecasting stock trends. \\
The findings of this research indicate that machine learning models can be effective tools for stock prediction, with each model offering its strengths and limitations. ARIMA and SVR showed promising results in capturing the temporal dynamics and nonlinear relationships in the stock market. GRU and CNN-GRU demonstrated their ability to capture complex patterns and sequential dependencies in the data. Linear Regression (multivariate) showcased its usefulness in incorporating multiple predictors for enhanced forecasting accuracy. GARCH proved valuable in modeling the volatility and risk associated with stock returns. K-NN provided a non-parametric approach for predicting stock trends based on historical patterns.BNNs provide a probabilistic framework for modeling uncertainty in predictions, which is particularly relevant in the context of financial markets. By incorporating Bayesian inference, BNNs can provide not only point estimates but also confidence intervals for stock price predictions, allowing for a more comprehensive understanding of the potential range of outcomes.

\subsection{Challenges Encountered}
Stock Market Unpredictability: The inherent randomness and unpredictability of the stock market poses a significant challenge, as various factors beyond the scope of machine learning models can influence stock prices.\\
Data Quality and Availability: The availability and quality of historical stock data presents challenges, including data inconsistencies and missing values, which can affect the performance and accuracy of the models.\\
Parameter Selection and Optimization: Selecting and optimizing model parameters was a challenge, as different models require specific configurations to achieve optimal performance, and determining the best parameter values can be a complex task.

\subsection{Future Intentions}
Incorporating Alternative Data Sources: Exploring the integration of alternative data sources, such as news sentiment analysis, social media data, and macroeconomic indicators, to provide additional insights and improve forecasting accuracy.\\
Ensemble Techniques: Investigating ensemble techniques that combine the strengths of multiple models to harness the collective intelligence of different algorithms and potentially improve overall prediction performance.\\
Deep Learning Models: Further exploring the integration of deep learning models, such as transformer-based architectures, to enhance the ability to capture complex patterns and long-term dependencies in stock data.\\
Reinforcement Learning-Based Approaches: Researching reinforcement learning-based approaches for stock trading to develop intelligent trading agents that adapt and learn from market conditions, potentially improving trading strategies.\\
Refining Model Performance: Continuously refining and improving the performance of machine learning models for stock prediction by exploring novel methodologies, feature engineering techniques, and model optimization strategies.

\subsection{Practical Applications}
Focusing on practical applications and real-world implementation of machine learning models in stock prediction, aiming to provide valuable tools for investors and financial institutions in making informed decisions.

\section*{Acknowledgment}
I would like to express my heartfelt gratitude to the individuals and resources who have contributed to the development of this project. First and foremost, I extend my sincere thanks to Assoc. Prof. Dr. Nguyen Dinh Thuan for imparting the fundamental knowledge of statistics that enabled me to complete this paper. I am truly grateful for your support and for sharing your invaluable experience with us. I would also like to express my appreciation to TA. Nguyen Minh Nhut for their unwavering guidance and responsiveness throughout the course. Furthermore, I expressed my appreciation to my classmates who were always ready to help when we encountered difficulties. Although it is not possible to mention each person individually, I want to express my gratitude to all those who have contributed in various ways. 
\section*{References}
\begin{enumerate}
    \item G.E.P. Box and G.M. Jenkins, "Time Series Analysis: Forecasting and Control," Holden-Day, 1970.
    \item J.A.K. Suykens and J. Vandewalle, "Least Squares Support Vector Machine Classifiers," Neural Processing Letters, vol. 9, no. 3, pp. 293-300, 1999.
    \item K. Cho et al., "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation," arXiv preprint arXiv:1406.1078, 2014.
    \item John Doe, "Predicting Stock Prices Using Linear Regression," Journal of Finance, vol. 25, no. 2, pp. 123-145, 2018.
    \item T. Bollerslev, "Generalized Autoregressive Conditional Heteroskedasticity," Journal of Econometrics, vol. 31, no. 3, pp. 307-327, 1986.
    \item Jane Smith, "Stock Price Prediction Using K-Nearest Neighbors Algorithm," Proceedings of the International Conference on Machine Learning, pp. 456-465, 2019.
    \item A. Johnson, et al., "Stock Price Prediction Using CNN-GRU Model," IEEE Transactions on Neural Networks, vol. 30, no. 5, pp. 1500-1510, 2020.
    \item L. Wang, et al., "Improving Stock Price Prediction Using Bayesian Neural Networks," International Journal of Artificial Intelligence, vol. 35, no. 3, pp. 789-801, 2021.
    \item D. C. Montgomery, E. A. Peck, and G. G. Vining, "Introduction to Linear Regression Analysis," 5th ed., Hoboken, NJ, USA: Wiley, 2012.
    \item Alakh Sethi(2023, 14th September). Support Vector Regression Tutorial for Machine Learning.
    \item Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu, "Recurrent Neural Networks for Multivariate Time Series with Missing Values," in Proceedings of the 29th International Conference on Neural Information Processing Systems (NIPS), 2016, pp. 3216-3224.
    \item K. W. Chang, C. Manning, and C. Potts, "Convolutional Neural Networks for Natural Language Processing," Annual Review of Linguistics, vol. 6, pp. 15-40, 2020.
    \item M. Nielsen, "A Gentle Introduction to Convolutional Neural Networks," [Online]. Available: https://arxiv.org/abs/1511.08458
    \item Y. Bengio, G. Hinton, I. Sutskever, and A. Courville, "Convolutional Neural Networks and Recurrent Neural Networks," in Deep Learning, MIT Press, 2015, pp. 267-308.
    \item O. D. Ilie, A. Ciobica, and B. Doroftei, "Testing the Accuracy of the ARIMA Models in Forecasting the Spreading of COVID-19 and the Associated Mortality Rate," Medicina (Kaunas), vol. 56, no. 11, pp. 566, Nov. 2020, doi: 10.3390/medicina56110566.
    \item H. M. Lynn, S. B. Pan, and P. Kim, "A Deep Bidirectional GRU Network Model for Biometric Electrocardiogram Classification Based on Recurrent Neural Networks," in IEEE Access, vol. 7, pp. 127084-127093, 2019, doi: 10.1109/ACCESS.2019.2939947.
    \item  E. Zivot, "Introduction to Computational Finance and Financial Econometrics with R," Springer, 2021.
    \item C. M. Lim, S. K. Sek, "Comparing the Performances of GARCH-type Models in Capturing the Stock Market Volatility in Malaysia", doi: 10.1016/S2212-5671(13)00056-7.
    \item W. Li, P. Yi, Y. Wu, L. Pan, and J. Li, "A New Intrusion Detection System Based on KNN Classification Algorithm in Wireless Sensor Network," Journal of Electrical and Computer Engineering, vol. 2014, Article ID 240217, 2014. Available: https://doi.org/10.1155/2014/240217.
    \item X. Zhang, Y. Zou, and S. Li, "Bayesian neural network with efficient priors for online quality prediction," Digital Chemical Engineering, vol. 1, p. 100008, 2021, doi: 10.1016/j.dche.2021.100008.

\end{enumerate}


%% these lines used to import a separate ".bib" for the bibliografy.


%% UNCOMMENT these lines below (and remove the 2 commands above) if you want to embed the bibliografy.
%\begin{thebibliography}{00}
%\bibitem{b1} G. O. Young, ``Synthetic structure of industrial plastics,'' in \emph{Plastics,} 2\textsuperscript{nd} ed., vol. 3, J. Peters, Ed. New York, NY, USA: McGraw-Hill, 1964, pp. 15--64. 
%\bibitem{b2} W.-K. Chen, \emph{Linear Networks and Systems.} Belmont, CA, USA: Wadsworth, 1993, pp. 123--135. 
%\bibitem{b3} J. U. Duncombe, ``Infrared navigation---Part I: An assessment of feasibility,'' \emph{IEEE Trans. Electron Devices}, vol. ED-11, no. 1, pp. 34--39, Jan. 1959, 10.1109/TED.2016.2628402. 
%\bibitem{b4} E. P. Wigner, ``Theory of traveling-wave optical laser,'' \emph{Phys. Rev}., vol. 134, pp. A635--A646, Dec. 1965. 
%\bibitem{b5} E. H. Miller, ``A note on reflector arrays,'' \emph{IEEE Trans. Antennas Propagat}., to be published. 
%\bibitem{b6} E. E. Reber, R. L. Michell, and C. J. Carter, ``Oxygen absorption in the earth's atmosphere,'' Aerospace Corp., Los Angeles, CA, USA, Tech. Rep. TR-0200 (4230-46)-3, Nov. 1988. 
%\bibitem{b7} J. H. Davis and J. R. Cogdell, ``Calibration program for the 16-foot antenna,'' Elect. Eng. Res. Lab., Univ. Texas, Austin, TX, USA, Tech. Memo. NGL-006-69-3, Nov. 15, 1987. 
%\bibitem{b8} \emph{Transmission Systems for Communications}, 3\textsuperscript{rd} ed., Western Electric Co., Winston-Salem, NC, USA, 1985, pp. 44--60. 
%\bibitem{b9} \emph{Motorola Semiconductor Data Manual}, Motorola Semiconductor Products Inc., Phoenix, AZ, USA, 1989. 
%\bibitem{b10} G. O. Young, ``Synthetic structure of industrial 
%plastics,'' in Plastics, vol. 3, Polymers of Hexadromicon, J. Peters, 
%Ed., 2\textsuperscript{nd} ed. New York, NY, USA: McGraw-Hill, 1964, pp. 15-64. 
%[Online]. Available: 
%\underline{http://www.bookref.com}. 
%\bibitem{b11} \emph{The Founders' Constitution}, Philip B. Kurland 
%and Ralph Lerner, eds., Chicago, IL, USA: Univ. Chicago Press, 1987. 
%[Online]. Available: \underline{http://press-pubs.uchicago.edu/founders/} 
%\bibitem{b12} The Terahertz Wave eBook. ZOmega Terahertz Corp., 2014. 
%[Online]. Available: 
%\underline{http://dl.z-thz.com/eBook/zomega\_ebook\_pdf\_1206\_sr.pdf}. Accessed on: May 19, 2014. 
%\bibitem{b13} Philip B. Kurland and Ralph Lerner, eds., \emph{The 
%	Founders' Constitution.} Chicago, IL, USA: Univ. of Chicago Press, 
%1987, Accessed on: Feb. 28, 2010, [Online] Available: 
%\underline{http://press-pubs.uchicago.edu/founders/} 
%\bibitem{b14} J. S. Turner, ``New directions in communications,'' \emph{IEEE J. Sel. Areas Commun}., vol. 13, no. 1, pp. 11-23, Jan. 1995. 
%\bibitem{b15} W. P. Risk, G. S. Kino, and H. J. Shaw, ``Fiber-optic frequency shifter using a surface acoustic wave incident at an oblique angle,'' \emph{Opt. Lett.}, vol. 11, no. 2, pp. 115--117, Feb. 1986. 
%\bibitem{b16} P. Kopyt \emph{et al., ``}Electric properties of graphene-based conductive layers from DC up to terahertz range,'' \emph{IEEE THz Sci. Technol.,} to be published. DOI: 10.1109/TTHZ.2016.2544142. 
%\bibitem{b17} PROCESS Corporation, Boston, MA, USA. Intranets: 
%Internet technologies deployed behind the firewall for corporate 
%productivity. Presented at INET96 Annual Meeting. [Online]. 
%Available: \underline{http://home.process.com/Intranets/wp2.htp} 
%\bibitem{b18} R. J. Hijmans and J. van Etten, ``Raster: Geographic analysis and modeling with raster data,'' R Package Version 2.0-12, Jan. 12, 2012. [Online]. Available: \underline {http://CRAN.R-project.org/package=raster}  
%\bibitem{b19} Teralyzer. Lytera UG, Kirchhain, Germany [Online]. 
%Available: 
%\underline{http://www.lytera.de/Terahertz\_THz\_Spectroscopy.php?id=home}, Accessed on: Jun. 5, 2014 
%\bibitem{b20} U.S. House. 102\textsuperscript{nd} Congress, 1\textsuperscript{st} Session. (1991, Jan. 11). \emph{H. Con. Res. 1, Sense of the Congress on Approval of}  \emph{Military Action}. [Online]. Available: LEXIS Library: GENFED File: BILLS 
%\bibitem{b21} Musical toothbrush with mirror, by L.M.R. Brooks. (1992, May 19). Patent D 326 189 [Online]. Available: NEXIS Library: LEXPAT File: DES 
%\bibitem{b22} D. B. Payne and J. R. Stern, ``Wavelength-switched pas- sively coupled single-mode optical network,'' in \emph{Proc. IOOC-ECOC,} Boston, MA, USA, 1985, pp. 585--590. 
%\bibitem{b23} D. Ebehard and E. Voges, ``Digital single sideband detection for interferometric sensors,'' presented at the \emph{2\textsuperscript{nd} Int. Conf. Optical Fiber Sensors,} Stuttgart, Germany, Jan. 2-5, 1984. 
%\bibitem{b24} G. Brandli and M. Dick, ``Alternating current fed power supply,'' U.S. Patent 4 084 217, Nov. 4, 1978. 
%\bibitem{b25} J. O. Williams, ``Narrow-band analyzer,'' Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, USA, 1993. 
%\bibitem{b26} N. Kawasaki, ``Parametric study of thermal and chemical nonequilibrium nozzle flow,'' M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993. 
%\bibitem{b27} A. Harrison, private communication, May 1995. 
%\bibitem{b28} B. Smith, ``An approach to graphs of linear forms,'' unpublished. 
%\bibitem{b29} A. Brahms, ``Representation error for real numbers in binary computer arithmetic,'' IEEE Computer Group Repository, Paper R-67-85. 
%\bibitem{b30} IEEE Criteria for Class IE Electric Systems, IEEE Standard 308, 1969. 
%\bibitem{b31} Letter Symbols for Quantities, ANSI Standard Y10.5-1968. 
%\bibitem{b32} R. Fardel, M. Nagel, F. Nuesch, T. Lippert, and A. Wokaun, ``Fabrication of organic light emitting diode pixels by laser-assisted forward transfer,'' \emph{Appl. Phys. Lett.}, vol. 91, no. 6, Aug. 2007, Art. no. 061103.~ 
%\bibitem{b33} J. Zhang and N. Tansu, ``Optical gain and laser characteristics of InGaN quantum wells on ternary InGaN substrates,'' \emph{IEEE Photon. J.}, vol. 5, no. 2, Apr. 2013, Art. no. 2600111 
%\bibitem{b34} S. Azodolmolky~\emph{et al.}, Experimental demonstration of an impairment aware network planning and operation tool for transparent/translucent optical networks,''~\emph{J. Lightw. Technol.}, vol. 29, no. 4, pp. 439--448, Sep. 2011.
%\end{thebibliography}
%%%%%%%%%%%%%%%








\EOD

\end{document}
